Did you ever realize that we are stuck optimizing our life? Be it College Selection when we give our entrance exams, job switching, food choices. We always try to get that "sweet spot" but no matter how much we try, the optimization doesnt happen optimally. 
Like , lets take eating healthy, we just deviate from the optimized path to eat food which we love, which we know is not good for us, yet we indulge in them in the name of "guilty pleasure"
Maybe, just maybe, ML models also require a bit of not-so-fully-optimized weights and biases. 

Maybe that one edge case is taken care of when we have non-optimal optimization. Maybe we get a higher model like that.
Perfection isn't the goal, generalization is , in the context of ML Models.
